{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fc1278-6831-40ae-a705-59bea3c83a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"REDACTEDproj-sgRbY5gLQ7MLNRdyk44_gOHJ2JXXx4curgglvhI1vSTRqjVqHNrsesR5vpxu5gRsv6PGB-IDM5T3BlbkFJSSDwEtP8nm-HHpKqoqiA2Njs0MCh8AEYRd4Fl4qrIzVnkZ6aFxJ_3i2Q6_AI37zzTTynuE9moA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148ef4fc-304f-4b5f-8669-fe22805d506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] OPENAI_API_KEY detected.\n",
      "[INFO] Embedding model set to: text-embedding-3-small\n",
      "[INFO] Setup helpers loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup & Sanity -----------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "# If you put OPENAI_API_KEY in a .env file, load it:\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Basic logging (aligned with your post-4 emphasis on evaluation/logs)\n",
    "logger = logging.getLogger(\"rag-setup\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))\n",
    "    logger.addHandler(h)\n",
    "\n",
    "def ensure_dirs(paths: List[str]) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "ensure_dirs([\"data\", \"indexes\", \"docs\"])\n",
    "\n",
    "# Validate OpenAI key availability\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY or not OPENAI_API_KEY.strip():\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY is missing. Set it via os.environ or in a .env file.\"\n",
    "    )\n",
    "logger.info(\"OPENAI_API_KEY detected.\")\n",
    "\n",
    "# OpenAI client (new SDK)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"OpenAI SDK not found. Run: pip install openai==1.*\"\n",
    "    ) from e\n",
    "\n",
    "client = OpenAI()  # reads API key from env by default\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # fast, cheap, strong baseline\n",
    "logger.info(f\"Embedding model set to: {EMBED_MODEL}\")\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get embeddings for a list of strings using OpenAI embeddings.\n",
    "    Returns a 2D numpy array [n_texts, dim].\n",
    "    \"\"\"\n",
    "    if not isinstance(texts, list) or not texts or not all(isinstance(t, str) and t.strip() for t in texts):\n",
    "        raise ValueError(\"texts must be a non-empty list of non-empty strings.\")\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vecs = [d.embedding for d in resp.data]\n",
    "    # Normalize to unit length → inner product == cosine similarity\n",
    "    arr = np.array(vecs, dtype=np.float32)\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    return arr / norms\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cosine similarity in [-1, 1], assuming both vectors are already normalized.\n",
    "    \"\"\"\n",
    "    if a.ndim != 1 or b.ndim != 1 or a.shape[0] != b.shape[0]:\n",
    "        raise ValueError(\"Inputs must be 1D vectors of the same length.\")\n",
    "    return float(np.dot(a, b))  # with normalized vectors, dot == cosine\n",
    "\n",
    "logger.info(\"Setup helpers loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592eae88-65e0-4c91-b181-993a693444f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"pair\": [\n",
      "      \"heart attack\",\n",
      "      \"myocardial infarction\"\n",
      "    ],\n",
      "    \"cosine_similarity\": 0.591\n",
      "  },\n",
      "  {\n",
      "    \"pair\": [\n",
      "      \"hypertension\",\n",
      "      \"high blood pressure\"\n",
      "    ],\n",
      "    \"cosine_similarity\": 0.783\n",
      "  },\n",
      "  {\n",
      "    \"pair\": [\n",
      "      \"antibiotic resistance\",\n",
      "      \"bacteria resistant to antibiotics\"\n",
      "    ],\n",
      "    \"cosine_similarity\": 0.734\n",
      "  },\n",
      "  {\n",
      "    \"pair\": [\n",
      "      \"diabetes\",\n",
      "      \"type 2 diabetes\"\n",
      "    ],\n",
      "    \"cosine_similarity\": 0.715\n",
      "  },\n",
      "  {\n",
      "    \"pair\": [\n",
      "      \"appendix\",\n",
      "      \"banana\"\n",
      "    ],\n",
      "    \"cosine_similarity\": 0.257\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Embedding Smoke Test -----------------------------------------\n",
    "pairs = [\n",
    "    (\"heart attack\", \"myocardial infarction\"),\n",
    "    (\"hypertension\", \"high blood pressure\"),\n",
    "    (\"antibiotic resistance\", \"bacteria resistant to antibiotics\"),\n",
    "    (\"diabetes\", \"type 2 diabetes\"),\n",
    "    (\"appendix\", \"banana\")  # control: unrelated terms\n",
    "]\n",
    "\n",
    "left = [p[0] for p in pairs]\n",
    "right = [p[1] for p in pairs]\n",
    "\n",
    "# Embed both sides\n",
    "L = embed_texts(left)\n",
    "R = embed_texts(right)\n",
    "\n",
    "# Compute pairwise cosine\n",
    "sims = [cosine_similarity(L[i], R[i]) for i in range(len(pairs))]\n",
    "\n",
    "results = [\n",
    "    {\"pair\": pairs[i], \"cosine_similarity\": round(sims[i], 3)}\n",
    "    for i in range(len(pairs))\n",
    "]\n",
    "\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c77788d-7766-445d-b13f-3198a054a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching PubMed abstracts for: antimicrobial resistance\n",
      "Downloading antimicrobial resistance: 100%|███| 120/120 [00:49<00:00,  2.44it/s]\n",
      "[INFO] Fetching PubMed abstracts for: hypertension\n",
      "Downloading hypertension: 100%|███████████████| 120/120 [00:50<00:00,  2.40it/s]\n",
      "[INFO] Fetched 233 total abstracts.\n",
      "[INFO] Saved → data/pubmed_corpus.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global susceptibility profiles and potential r...</td>\n",
       "      <td>A total of 2361 ceftazidime-avibactam-resistan...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/41054939/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advances in Isothermal Amplification for the D...</td>\n",
       "      <td>Detecting tuberculosis (TB) remains challengin...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/41054873/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amoxicillin/clavulanate as a cornerstone of an...</td>\n",
       "      <td>Antimicrobial resistance (AMR) remains a criti...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/41054784/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laboratory evaluation of 185 commercial assays...</td>\n",
       "      <td>In August 2020, Public Health England and Oxfo...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/41054442/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Evaluation of SARS-CoV-2 lateral flow device s...</td>\n",
       "      <td>Self-testing for SARS-CoV-2 infection using la...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/41054440/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Global susceptibility profiles and potential r...   \n",
       "1  Advances in Isothermal Amplification for the D...   \n",
       "2  Amoxicillin/clavulanate as a cornerstone of an...   \n",
       "3  Laboratory evaluation of 185 commercial assays...   \n",
       "4  Evaluation of SARS-CoV-2 lateral flow device s...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  A total of 2361 ceftazidime-avibactam-resistan...   \n",
       "1  Detecting tuberculosis (TB) remains challengin...   \n",
       "2  Antimicrobial resistance (AMR) remains a criti...   \n",
       "3  In August 2020, Public Health England and Oxfo...   \n",
       "4  Self-testing for SARS-CoV-2 infection using la...   \n",
       "\n",
       "                                         url  \n",
       "0  https://pubmed.ncbi.nlm.nih.gov/41054939/  \n",
       "1  https://pubmed.ncbi.nlm.nih.gov/41054873/  \n",
       "2  https://pubmed.ncbi.nlm.nih.gov/41054784/  \n",
       "3  https://pubmed.ncbi.nlm.nih.gov/41054442/  \n",
       "4  https://pubmed.ncbi.nlm.nih.gov/41054440/  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 3: Fetch PubMed Data ---------------------------------------------\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configure Entrez (PubMed API)\n",
    "Entrez.email = \"your_email@example.com\"  # required by NCBI, any valid email\n",
    "SEARCH_TERMS = [\"antimicrobial resistance\", \"hypertension\"]  # you can change topics\n",
    "MAX_RESULTS_PER_TERM = 120   # ~240 total (adjustable)\n",
    "\n",
    "def fetch_pubmed_abstracts(term: str, max_results: int = 100):\n",
    "    \"\"\"Search PubMed and return a list of (title, abstract, url).\"\"\"\n",
    "    logger.info(f\"Fetching PubMed abstracts for: {term}\")\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=term, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    ids = record[\"IdList\"]\n",
    "    results = []\n",
    "\n",
    "    for pmid in tqdm(ids, desc=f\"Downloading {term}\", ncols=80):\n",
    "        try:\n",
    "            fetch = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"xml\")\n",
    "            paper = Entrez.read(fetch)\n",
    "            art = paper[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"]\n",
    "            title = art.get(\"ArticleTitle\", \"\")\n",
    "            abstract = \" \".join([a for a in art.get(\"Abstract\", {}).get(\"AbstractText\", [])])\n",
    "            url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "            if title and abstract:\n",
    "                results.append({\"title\": title.strip(),\n",
    "                                \"abstract\": abstract.strip(),\n",
    "                                \"url\": url})\n",
    "            time.sleep(0.1)  # be polite to NCBI servers\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipped {pmid}: {e}\")\n",
    "    return results\n",
    "\n",
    "# Run for both terms\n",
    "all_records = []\n",
    "for term in SEARCH_TERMS:\n",
    "    all_records.extend(fetch_pubmed_abstracts(term, MAX_RESULTS_PER_TERM))\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "logger.info(f\"Fetched {len(df)} total abstracts.\")\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(\"data/pubmed_corpus.csv\", index=False)\n",
    "logger.info(\"Saved → data/pubmed_corpus.csv\")\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f775aba-6f74-4c11-b485-0a5ce23e2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 233 abstracts for chunking.\n",
      "Chunking: 100%|████████████████████████████| 233/233 [00:00<00:00, 30041.28it/s]\n",
      "[INFO] Total chunks created: 636\n",
      "[INFO] Embedding all chunks (this may take a minute)...\n",
      "Embedding: 100%|████████████████████████████████| 13/13 [00:18<00:00,  1.44s/it]\n",
      "[INFO] FAISS index + corpus metadata saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Chunk + Embed + Build FAISS Index ------------------------------\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the abstracts\n",
    "df = pd.read_csv(\"data/pubmed_corpus.csv\")\n",
    "logger.info(f\"Loaded {len(df)} abstracts for chunking.\")\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into overlapping chunks (character-level).\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += size - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Build list of chunks with metadata\n",
    "records = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), ncols=80, desc=\"Chunking\"):\n",
    "    for chunk in chunk_text(row[\"abstract\"]):\n",
    "        records.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"text\": chunk,\n",
    "            \"url\": row[\"url\"]\n",
    "        })\n",
    "\n",
    "logger.info(f\"Total chunks created: {len(records)}\")\n",
    "\n",
    "# Get embeddings (OpenAI)\n",
    "texts = [r[\"text\"] for r in records]\n",
    "logger.info(\"Embedding all chunks (this may take a minute)...\")\n",
    "BATCH = 50\n",
    "vecs = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH), ncols=80, desc=\"Embedding\"):\n",
    "    batch = texts[i:i+BATCH]\n",
    "    resp = client.embeddings.create(model=\"text-embedding-3-small\", input=batch)\n",
    "    batch_vecs = [d.embedding for d in resp.data]\n",
    "    vecs.extend(batch_vecs)\n",
    "\n",
    "embeddings = np.array(vecs, dtype=\"float32\")\n",
    "# Normalize for cosine similarity\n",
    "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "os.makedirs(\"indexes\", exist_ok=True)\n",
    "faiss.write_index(index, \"indexes/pubmed.index\")\n",
    "\n",
    "with open(\"indexes/corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(\"FAISS index + corpus metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "302e7c9c-b0f0-44ab-a294-90c66d892e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636\n",
      "636\n"
     ]
    }
   ],
   "source": [
    "print(index.ntotal)\n",
    "print(len(records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "094105f4-b2b5-4c65-978b-e0c905689515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Current challenges in combating antimicrobial resistance (AMR) '\n",
      "           'include the rapid emergence of multidrug-resistant (MDR) and '\n",
      "           'extensively drug-resistant (XDR) pathogens, which threaten modern '\n",
      "           'medical procedures such as surgery and chemotherapy. Conventional '\n",
      "           'antibiotics are increasingly limited due to mechanisms like '\n",
      "           'target-site mutations, efflux mechanisms, enzymatic degradation, '\n",
      "           'and biofilm-associated tolerance. Additionally, inappropriate '\n",
      "           'antimicrobial use accelerates AMR, creating tensions between '\n",
      "           'antimicrobial stewardship and timely treatment. There are also '\n",
      "           'systemic factors that contribute to gaps between recommendations '\n",
      "           'and actual clinical practice [Source 1][Source 2]. Furthermore, '\n",
      "           'global surveillance has shown a shift in infections from '\n",
      "           'staphylococcal dominance to multidrug-resistant Gram-negative '\n",
      "           'pathogens and fungi, particularly in low- and middle-income '\n",
      "           'countries where infection rates are higher [Source 3].',\n",
      " 'citations': [{'score': 0.61,\n",
      "                'title': 'Pseudomonas spp. and antimicrobial resistance: '\n",
      "                         'unlocking new horizons with 1-hydroxyphenazine.',\n",
      "                'url': 'https://pubmed.ncbi.nlm.nih.gov/41045408/'},\n",
      "               {'score': 0.575,\n",
      "                'title': 'The latent system factors that influence '\n",
      "                         'antimicrobial use and governance in healthcare: a '\n",
      "                         'scoping review of high-income health systems.',\n",
      "                'url': 'https://pubmed.ncbi.nlm.nih.gov/41048667/'},\n",
      "               {'score': 0.568,\n",
      "                'title': 'Evolving Patterns of Etiological Profiles and '\n",
      "                         'Susceptibility Trends of Healthcare-Associated '\n",
      "                         'Infections Since Inception: A Meta-Analysis.',\n",
      "                'url': 'https://pubmed.ncbi.nlm.nih.gov/41050025/'}],\n",
      " 'quality': 0.584,\n",
      " 'question': 'What are current challenges in combating antimicrobial '\n",
      "             'resistance?'}\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Retrieval + Answer Generation -----------------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Load FAISS + corpus\n",
    "index = faiss.read_index(\"indexes/pubmed.index\")\n",
    "with open(\"indexes/corpus.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "def retrieve_top_k(question: str, k: int = 3):\n",
    "    \"\"\"Retrieve top-k chunks for a question.\"\"\"\n",
    "    q_vec = client.embeddings.create(model=\"text-embedding-3-small\", input=[question]).data[0].embedding\n",
    "    q_vec = np.array(q_vec, dtype=\"float32\")\n",
    "    q_vec /= np.linalg.norm(q_vec)\n",
    "    scores, idxs = index.search(q_vec.reshape(1, -1), k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"title\": corpus[idx][\"title\"],\n",
    "            \"text\": corpus[idx][\"text\"],\n",
    "            \"url\": corpus[idx][\"url\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def build_prompt(question: str, contexts: list[dict]):\n",
    "    \"\"\"Build a structured RAG prompt with clear boundaries + citation requirement.\"\"\"\n",
    "    context_texts = []\n",
    "    for i, ctx in enumerate(contexts, start=1):\n",
    "        context_texts.append(f\"[Source {i}] {ctx['text']}\\n(Citation: {ctx['url']})\")\n",
    "    joined_context = \"\\n\\n\".join(context_texts)\n",
    "    prompt = f\"\"\"\n",
    "You are a medical research assistant.\n",
    "Use ONLY the following retrieved context to answer the question.\n",
    "If the answer cannot be found, say \"Not enough information in the retrieved papers.\"\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Retrieved Context:\n",
    "{joined_context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer concisely and factually.\n",
    "2. Cite sources by [Source #].\n",
    "3. Do not include information not in the context.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_answer(question: str, k: int = 3, model: str = \"gpt-4o-mini\"):\n",
    "    \"\"\"Full RAG pipeline: retrieve, prompt, generate.\"\"\"\n",
    "    hits = retrieve_top_k(question, k=k)\n",
    "    prompt = build_prompt(question, hits)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    quality = round(np.mean([h[\"score\"] for h in hits]), 3)\n",
    "    citations = [{\"title\": h[\"title\"], \"url\": h[\"url\"], \"score\": round(h[\"score\"], 3)} for h in hits]\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"quality\": quality,\n",
    "        \"citations\": citations\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Example query\n",
    "query = \"What are current challenges in combating antimicrobial resistance?\"\n",
    "result = generate_answer(query, k=3)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
